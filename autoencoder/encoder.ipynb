{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Resize((64, 64)),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "anime_data = datasets.MNIST(\n",
    "    root=\"./data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transform,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = torch.utils.data.DataLoader(\n",
    "    dataset=anime_data, batch_size=64, shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearAutoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LinearAutoencoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(64 * 64, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 12),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(12, 3),\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(3, 12),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(12, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64 * 64),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = LinearAutoencoder().to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bored/.local/lib/python3.11/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Loss: 0.0576\n",
      "Epoch: 2, Loss: 0.0582\n",
      "Epoch: 3, Loss: 0.0588\n",
      "Epoch: 4, Loss: 0.0605\n",
      "Epoch: 5, Loss: 0.0630\n",
      "Epoch: 6, Loss: 0.0578\n",
      "Epoch: 7, Loss: 0.0526\n",
      "Epoch: 8, Loss: 0.0597\n",
      "Epoch: 9, Loss: 0.0554\n",
      "Epoch: 10, Loss: 0.0606\n",
      "Epoch: 11, Loss: 0.0605\n",
      "Epoch: 12, Loss: 0.0550\n",
      "Epoch: 13, Loss: 0.0565\n",
      "Epoch: 14, Loss: 0.0565\n",
      "Epoch: 15, Loss: 0.0645\n",
      "Epoch: 16, Loss: 0.0562\n",
      "Epoch: 17, Loss: 0.0573\n",
      "Epoch: 18, Loss: 0.0538\n",
      "Epoch: 19, Loss: 0.0585\n",
      "Epoch: 20, Loss: 0.0581\n",
      "Epoch: 21, Loss: 0.0626\n",
      "Epoch: 22, Loss: 0.0585\n",
      "Epoch: 23, Loss: 0.0635\n",
      "Epoch: 24, Loss: 0.0571\n",
      "Epoch: 25, Loss: 0.0577\n",
      "Epoch: 26, Loss: 0.0550\n",
      "Epoch: 27, Loss: 0.0497\n",
      "Epoch: 28, Loss: 0.0531\n",
      "Epoch: 29, Loss: 0.0635\n",
      "Epoch: 30, Loss: 0.0569\n",
      "Epoch: 31, Loss: 0.0517\n",
      "Epoch: 32, Loss: 0.0545\n",
      "Epoch: 33, Loss: 0.0577\n",
      "Epoch: 34, Loss: 0.0586\n",
      "Epoch: 35, Loss: 0.0597\n",
      "Epoch: 36, Loss: 0.0543\n",
      "Epoch: 37, Loss: 0.0602\n",
      "Epoch: 38, Loss: 0.0577\n",
      "Epoch: 39, Loss: 0.0572\n",
      "Epoch: 40, Loss: 0.0593\n",
      "Epoch: 41, Loss: 0.0553\n",
      "Epoch: 42, Loss: 0.0589\n",
      "Epoch: 43, Loss: 0.0636\n",
      "Epoch: 44, Loss: 0.0531\n",
      "Epoch: 45, Loss: 0.0540\n",
      "Epoch: 46, Loss: 0.0609\n",
      "Epoch: 47, Loss: 0.0592\n",
      "Epoch: 48, Loss: 0.0600\n",
      "Epoch: 49, Loss: 0.0639\n",
      "Epoch: 50, Loss: 0.0561\n",
      "Epoch: 51, Loss: 0.0521\n",
      "Epoch: 52, Loss: 0.0556\n",
      "Epoch: 53, Loss: 0.0610\n",
      "Epoch: 54, Loss: 0.0602\n",
      "Epoch: 55, Loss: 0.0570\n",
      "Epoch: 56, Loss: 0.0581\n",
      "Epoch: 57, Loss: 0.0613\n",
      "Epoch: 58, Loss: 0.0554\n",
      "Epoch: 59, Loss: 0.0572\n",
      "Epoch: 60, Loss: 0.0572\n",
      "Epoch: 61, Loss: 0.0578\n",
      "Epoch: 62, Loss: 0.0568\n",
      "Epoch: 63, Loss: 0.0586\n",
      "Epoch: 64, Loss: 0.0562\n",
      "Epoch: 65, Loss: 0.0538\n",
      "Epoch: 66, Loss: 0.0586\n",
      "Epoch: 67, Loss: 0.0582\n",
      "Epoch: 68, Loss: 0.0571\n",
      "Epoch: 69, Loss: 0.0534\n",
      "Epoch: 70, Loss: 0.0597\n",
      "Epoch: 71, Loss: 0.0579\n",
      "Epoch: 72, Loss: 0.0507\n",
      "Epoch: 73, Loss: 0.0590\n",
      "Epoch: 74, Loss: 0.0573\n",
      "Epoch: 75, Loss: 0.0588\n",
      "Epoch: 76, Loss: 0.0570\n",
      "Epoch: 77, Loss: 0.0600\n",
      "Epoch: 78, Loss: 0.0574\n",
      "Epoch: 79, Loss: 0.0536\n",
      "Epoch: 80, Loss: 0.0596\n",
      "Epoch: 81, Loss: 0.0568\n",
      "Epoch: 82, Loss: 0.0561\n",
      "Epoch: 83, Loss: 0.0592\n",
      "Epoch: 84, Loss: 0.0564\n",
      "Epoch: 85, Loss: 0.0530\n",
      "Epoch: 86, Loss: 0.0603\n",
      "Epoch: 87, Loss: 0.0630\n",
      "Epoch: 88, Loss: 0.0546\n",
      "Epoch: 89, Loss: 0.0612\n",
      "Epoch: 90, Loss: 0.0591\n",
      "Epoch: 91, Loss: 0.0630\n",
      "Epoch: 92, Loss: 0.0554\n",
      "Epoch: 93, Loss: 0.0514\n",
      "Epoch: 94, Loss: 0.0579\n",
      "Epoch: 95, Loss: 0.0585\n",
      "Epoch: 96, Loss: 0.0572\n",
      "Epoch: 97, Loss: 0.0562\n",
      "Epoch: 98, Loss: 0.0656\n",
      "Epoch: 99, Loss: 0.0563\n",
      "Epoch: 100, Loss: 0.0615\n",
      "Epoch: 101, Loss: 0.0595\n",
      "Epoch: 102, Loss: 0.0568\n",
      "Epoch: 103, Loss: 0.0575\n",
      "Epoch: 104, Loss: 0.0617\n",
      "Epoch: 105, Loss: 0.0591\n",
      "Epoch: 106, Loss: 0.0561\n",
      "Epoch: 107, Loss: 0.0634\n",
      "Epoch: 108, Loss: 0.0531\n",
      "Epoch: 109, Loss: 0.0625\n",
      "Epoch: 110, Loss: 0.0532\n",
      "Epoch: 111, Loss: 0.0584\n",
      "Epoch: 112, Loss: 0.0654\n",
      "Epoch: 113, Loss: 0.0595\n",
      "Epoch: 114, Loss: 0.0573\n",
      "Epoch: 115, Loss: 0.0581\n",
      "Epoch: 116, Loss: 0.0583\n",
      "Epoch: 117, Loss: 0.0572\n",
      "Epoch: 118, Loss: 0.0615\n",
      "Epoch: 119, Loss: 0.0594\n",
      "Epoch: 120, Loss: 0.0578\n",
      "Epoch: 121, Loss: 0.0587\n",
      "Epoch: 122, Loss: 0.0554\n",
      "Epoch: 123, Loss: 0.0589\n",
      "Epoch: 124, Loss: 0.0523\n",
      "Epoch: 125, Loss: 0.0548\n",
      "Epoch: 126, Loss: 0.0546\n",
      "Epoch: 127, Loss: 0.0588\n",
      "Epoch: 128, Loss: 0.0592\n",
      "Epoch: 129, Loss: 0.0576\n",
      "Epoch: 130, Loss: 0.0635\n",
      "Epoch: 131, Loss: 0.0549\n",
      "Epoch: 132, Loss: 0.0547\n",
      "Epoch: 133, Loss: 0.0573\n",
      "Epoch: 134, Loss: 0.0537\n",
      "Epoch: 135, Loss: 0.0573\n",
      "Epoch: 136, Loss: 0.0518\n",
      "Epoch: 137, Loss: 0.0550\n",
      "Epoch: 138, Loss: 0.0617\n",
      "Epoch: 139, Loss: 0.0578\n",
      "Epoch: 140, Loss: 0.0569\n",
      "Epoch: 141, Loss: 0.0564\n",
      "Epoch: 142, Loss: 0.0597\n",
      "Epoch: 143, Loss: 0.0561\n",
      "Epoch: 144, Loss: 0.0564\n",
      "Epoch: 145, Loss: 0.0588\n",
      "Epoch: 146, Loss: 0.0580\n",
      "Epoch: 147, Loss: 0.0598\n",
      "Epoch: 148, Loss: 0.0548\n",
      "Epoch: 149, Loss: 0.0584\n",
      "Epoch: 150, Loss: 0.0603\n",
      "Epoch: 151, Loss: 0.0541\n",
      "Epoch: 152, Loss: 0.0575\n",
      "Epoch: 153, Loss: 0.0601\n",
      "Epoch: 154, Loss: 0.0601\n",
      "Epoch: 155, Loss: 0.0504\n",
      "Epoch: 156, Loss: 0.0527\n",
      "Epoch: 157, Loss: 0.0598\n",
      "Epoch: 158, Loss: 0.0563\n",
      "Epoch: 159, Loss: 0.0528\n",
      "Epoch: 160, Loss: 0.0576\n",
      "Epoch: 161, Loss: 0.0571\n",
      "Epoch: 162, Loss: 0.0587\n",
      "Epoch: 163, Loss: 0.0630\n",
      "Epoch: 164, Loss: 0.0582\n",
      "Epoch: 165, Loss: 0.0597\n",
      "Epoch: 166, Loss: 0.0542\n",
      "Epoch: 167, Loss: 0.0614\n",
      "Epoch: 168, Loss: 0.0597\n",
      "Epoch: 169, Loss: 0.0559\n",
      "Epoch: 170, Loss: 0.0575\n",
      "Epoch: 171, Loss: 0.0562\n",
      "Epoch: 172, Loss: 0.0560\n",
      "Epoch: 173, Loss: 0.0542\n",
      "Epoch: 174, Loss: 0.0527\n",
      "Epoch: 175, Loss: 0.0582\n",
      "Epoch: 176, Loss: 0.0569\n",
      "Epoch: 177, Loss: 0.0623\n",
      "Epoch: 178, Loss: 0.0577\n",
      "Epoch: 179, Loss: 0.0640\n",
      "Epoch: 180, Loss: 0.0586\n",
      "Epoch: 181, Loss: 0.0582\n",
      "Epoch: 182, Loss: 0.0584\n",
      "Epoch: 183, Loss: 0.0577\n",
      "Epoch: 184, Loss: 0.0618\n",
      "Epoch: 185, Loss: 0.0575\n",
      "Epoch: 186, Loss: 0.0601\n",
      "Epoch: 187, Loss: 0.0612\n",
      "Epoch: 188, Loss: 0.0556\n",
      "Epoch: 189, Loss: 0.0560\n",
      "Epoch: 190, Loss: 0.0625\n",
      "Epoch: 191, Loss: 0.0552\n",
      "Epoch: 192, Loss: 0.0521\n",
      "Epoch: 193, Loss: 0.0574\n",
      "Epoch: 194, Loss: 0.0558\n",
      "Epoch: 195, Loss: 0.0547\n",
      "Epoch: 196, Loss: 0.0556\n",
      "Epoch: 197, Loss: 0.0553\n",
      "Epoch: 198, Loss: 0.0538\n",
      "Epoch: 199, Loss: 0.0551\n",
      "Epoch: 200, Loss: 0.0552\n",
      "Epoch: 201, Loss: 0.0583\n",
      "Epoch: 202, Loss: 0.0604\n",
      "Epoch: 203, Loss: 0.0548\n",
      "Epoch: 204, Loss: 0.0569\n",
      "Epoch: 205, Loss: 0.0547\n",
      "Epoch: 206, Loss: 0.0562\n",
      "Epoch: 207, Loss: 0.0633\n",
      "Epoch: 208, Loss: 0.0578\n",
      "Epoch: 209, Loss: 0.0587\n",
      "Epoch: 210, Loss: 0.0604\n",
      "Epoch: 211, Loss: 0.0577\n",
      "Epoch: 212, Loss: 0.0670\n",
      "Epoch: 213, Loss: 0.0602\n",
      "Epoch: 214, Loss: 0.0587\n",
      "Epoch: 215, Loss: 0.0674\n",
      "Epoch: 216, Loss: 0.0610\n",
      "Epoch: 217, Loss: 0.0565\n",
      "Epoch: 218, Loss: 0.0620\n",
      "Epoch: 219, Loss: 0.0620\n",
      "Epoch: 220, Loss: 0.0534\n",
      "Epoch: 221, Loss: 0.0561\n",
      "Epoch: 222, Loss: 0.0540\n",
      "Epoch: 223, Loss: 0.0537\n",
      "Epoch: 224, Loss: 0.0560\n",
      "Epoch: 225, Loss: 0.0559\n",
      "Epoch: 226, Loss: 0.0556\n",
      "Epoch: 227, Loss: 0.0593\n",
      "Epoch: 228, Loss: 0.0562\n",
      "Epoch: 229, Loss: 0.0561\n",
      "Epoch: 230, Loss: 0.0561\n",
      "Epoch: 231, Loss: 0.0595\n",
      "Epoch: 232, Loss: 0.0560\n",
      "Epoch: 233, Loss: 0.0594\n",
      "Epoch: 234, Loss: 0.0605\n",
      "Epoch: 235, Loss: 0.0532\n",
      "Epoch: 236, Loss: 0.0602\n",
      "Epoch: 237, Loss: 0.0592\n",
      "Epoch: 238, Loss: 0.0586\n",
      "Epoch: 239, Loss: 0.0579\n",
      "Epoch: 240, Loss: 0.0582\n",
      "Epoch: 241, Loss: 0.0569\n",
      "Epoch: 242, Loss: 0.0591\n",
      "Epoch: 243, Loss: 0.0522\n",
      "Epoch: 244, Loss: 0.0526\n",
      "Epoch: 245, Loss: 0.0544\n",
      "Epoch: 246, Loss: 0.0599\n",
      "Epoch: 247, Loss: 0.0556\n",
      "Epoch: 248, Loss: 0.0561\n",
      "Epoch: 249, Loss: 0.0529\n",
      "Epoch: 250, Loss: 0.0577\n",
      "Epoch: 251, Loss: 0.0521\n",
      "Epoch: 252, Loss: 0.0560\n",
      "Epoch: 253, Loss: 0.0580\n",
      "Epoch: 254, Loss: 0.0590\n",
      "Epoch: 255, Loss: 0.0599\n",
      "Epoch: 256, Loss: 0.0545\n",
      "Epoch: 257, Loss: 0.0524\n",
      "Epoch: 258, Loss: 0.0535\n",
      "Epoch: 259, Loss: 0.0552\n",
      "Epoch: 260, Loss: 0.0583\n",
      "Epoch: 261, Loss: 0.0614\n",
      "Epoch: 262, Loss: 0.0550\n",
      "Epoch: 263, Loss: 0.0557\n",
      "Epoch: 264, Loss: 0.0638\n",
      "Epoch: 265, Loss: 0.0575\n",
      "Epoch: 266, Loss: 0.0535\n",
      "Epoch: 267, Loss: 0.0534\n",
      "Epoch: 268, Loss: 0.0580\n",
      "Epoch: 269, Loss: 0.0568\n",
      "Epoch: 270, Loss: 0.0635\n",
      "Epoch: 271, Loss: 0.0512\n",
      "Epoch: 272, Loss: 0.0534\n",
      "Epoch: 273, Loss: 0.0578\n",
      "Epoch: 274, Loss: 0.0645\n",
      "Epoch: 275, Loss: 0.0600\n",
      "Epoch: 276, Loss: 0.0572\n",
      "Epoch: 277, Loss: 0.0576\n",
      "Epoch: 278, Loss: 0.0633\n",
      "Epoch: 279, Loss: 0.0564\n",
      "Epoch: 280, Loss: 0.0572\n",
      "Epoch: 281, Loss: 0.0594\n",
      "Epoch: 282, Loss: 0.0600\n",
      "Epoch: 283, Loss: 0.0560\n",
      "Epoch: 284, Loss: 0.0615\n",
      "Epoch: 285, Loss: 0.0601\n",
      "Epoch: 286, Loss: 0.0574\n",
      "Epoch: 287, Loss: 0.0533\n",
      "Epoch: 288, Loss: 0.0558\n",
      "Epoch: 289, Loss: 0.0575\n",
      "Epoch: 290, Loss: 0.0553\n",
      "Epoch: 291, Loss: 0.0627\n",
      "Epoch: 292, Loss: 0.0541\n",
      "Epoch: 293, Loss: 0.0656\n",
      "Epoch: 294, Loss: 0.0608\n",
      "Epoch: 295, Loss: 0.0588\n",
      "Epoch: 296, Loss: 0.0571\n",
      "Epoch: 297, Loss: 0.0587\n",
      "Epoch: 298, Loss: 0.0516\n",
      "Epoch: 299, Loss: 0.0493\n",
      "Epoch: 300, Loss: 0.0524\n",
      "Epoch: 301, Loss: 0.0585\n",
      "Epoch: 302, Loss: 0.0579\n",
      "Epoch: 303, Loss: 0.0558\n",
      "Epoch: 304, Loss: 0.0533\n",
      "Epoch: 305, Loss: 0.0634\n",
      "Epoch: 306, Loss: 0.0588\n",
      "Epoch: 307, Loss: 0.0559\n",
      "Epoch: 308, Loss: 0.0579\n",
      "Epoch: 309, Loss: 0.0624\n",
      "Epoch: 310, Loss: 0.0573\n",
      "Epoch: 311, Loss: 0.0547\n",
      "Epoch: 312, Loss: 0.0607\n",
      "Epoch: 313, Loss: 0.0545\n",
      "Epoch: 314, Loss: 0.0584\n",
      "Epoch: 315, Loss: 0.0567\n",
      "Epoch: 316, Loss: 0.0604\n",
      "Epoch: 317, Loss: 0.0578\n",
      "Epoch: 318, Loss: 0.0598\n",
      "Epoch: 319, Loss: 0.0580\n",
      "Epoch: 320, Loss: 0.0609\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 320\n",
    "outputs=[]\n",
    "for epoch in range(num_epochs):\n",
    "    for img, _ in data_loader:\n",
    "        img = img.to(device)\n",
    "        img = img.view(img.size(0), -1)\n",
    "        recon = model(img)\n",
    "        loss = criterion(recon, img)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f\"Epoch: {epoch+1}, Loss: {loss.item():.4f}\")\n",
    "    outputs.append((epoch, img, recon))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), ``\"./models/linear-auto-encoder.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvAutoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvAutoencoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, 3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(16, 32, 3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, 3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 128, 3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(128, 64, 3, stride=2, padding=1, output_padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(64, 32, 3, stride=2, padding=1, output_padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(32, 16, 3, stride=2, padding=1, output_padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(16, 1, 3, stride=2, padding=1, output_padding=1),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = ConvAutoencoder().to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Loss: 0.0062\n",
      "0.0009338855743408203\n",
      "Epoch: 2, Loss: 0.0053\n",
      "0.0009436607360839844\n",
      "Epoch: 3, Loss: 0.0050\n",
      "0.0009496212005615234\n",
      "Epoch: 4, Loss: 0.0055\n",
      "0.0009670257568359375\n",
      "Epoch: 5, Loss: 0.0051\n",
      "0.0009510517120361328\n",
      "Epoch: 6, Loss: 0.0053\n",
      "0.0009949207305908203\n",
      "Epoch: 7, Loss: 0.0050\n",
      "0.0010416507720947266\n",
      "Epoch: 8, Loss: 0.0046\n",
      "0.0009200572967529297\n",
      "Epoch: 9, Loss: 0.0055\n",
      "0.000957489013671875\n",
      "Epoch: 10, Loss: 0.0054\n",
      "0.0009348392486572266\n",
      "Epoch: 11, Loss: 0.0054\n",
      "0.0009717941284179688\n",
      "Epoch: 12, Loss: 0.0050\n",
      "0.0009303092956542969\n",
      "Epoch: 13, Loss: 0.0048\n",
      "0.0009453296661376953\n",
      "Epoch: 14, Loss: 0.0047\n",
      "0.0009601116180419922\n",
      "Epoch: 15, Loss: 0.0052\n",
      "0.0009453296661376953\n",
      "Epoch: 16, Loss: 0.0050\n",
      "0.0009014606475830078\n",
      "Epoch: 17, Loss: 0.0052\n",
      "0.0009081363677978516\n",
      "Epoch: 18, Loss: 0.0050\n",
      "0.0009415149688720703\n",
      "Epoch: 19, Loss: 0.0045\n",
      "0.0009331703186035156\n",
      "Epoch: 20, Loss: 0.0048\n",
      "0.0009312629699707031\n",
      "Epoch: 21, Loss: 0.0052\n",
      "0.0009417533874511719\n",
      "Epoch: 22, Loss: 0.0044\n",
      "0.0009047985076904297\n",
      "Epoch: 23, Loss: 0.0046\n",
      "0.0009641647338867188\n",
      "Epoch: 24, Loss: 0.0046\n",
      "0.0009262561798095703\n",
      "Epoch: 25, Loss: 0.0049\n",
      "0.000926971435546875\n",
      "Epoch: 26, Loss: 0.0049\n",
      "0.0009167194366455078\n",
      "Epoch: 27, Loss: 0.0056\n",
      "0.0008940696716308594\n",
      "Epoch: 28, Loss: 0.0055\n",
      "0.0009033679962158203\n",
      "Epoch: 29, Loss: 0.0049\n",
      "0.0009317398071289062\n",
      "Epoch: 30, Loss: 0.0049\n",
      "0.0009019374847412109\n",
      "Epoch: 31, Loss: 0.0054\n",
      "0.0009620189666748047\n",
      "Epoch: 32, Loss: 0.0048\n",
      "0.000942230224609375\n",
      "Epoch: 33, Loss: 0.0055\n",
      "0.0009589195251464844\n",
      "Epoch: 34, Loss: 0.0052\n",
      "0.0009713172912597656\n",
      "Epoch: 35, Loss: 0.0051\n",
      "0.0010662078857421875\n",
      "Epoch: 36, Loss: 0.0047\n",
      "0.0009500980377197266\n",
      "Epoch: 37, Loss: 0.0045\n",
      "0.0009493827819824219\n",
      "Epoch: 38, Loss: 0.0052\n",
      "0.0009377002716064453\n",
      "Epoch: 39, Loss: 0.0049\n",
      "0.0009837150573730469\n",
      "Epoch: 40, Loss: 0.0048\n",
      "0.0009195804595947266\n",
      "Epoch: 41, Loss: 0.0046\n",
      "0.0009596347808837891\n",
      "Epoch: 42, Loss: 0.0045\n",
      "0.0009348392486572266\n",
      "Epoch: 43, Loss: 0.0053\n",
      "0.0009343624114990234\n",
      "Epoch: 44, Loss: 0.0046\n",
      "0.0009202957153320312\n",
      "Epoch: 45, Loss: 0.0045\n",
      "0.0009341239929199219\n",
      "Epoch: 46, Loss: 0.0053\n",
      "0.0009508132934570312\n",
      "Epoch: 47, Loss: 0.0048\n",
      "0.0009045600891113281\n",
      "Epoch: 48, Loss: 0.0053\n",
      "0.0008988380432128906\n",
      "Epoch: 49, Loss: 0.0042\n",
      "0.0009696483612060547\n",
      "Epoch: 50, Loss: 0.0046\n",
      "0.0009286403656005859\n",
      "Epoch: 51, Loss: 0.0050\n",
      "0.0009236335754394531\n",
      "Epoch: 52, Loss: 0.0049\n",
      "0.0009496212005615234\n",
      "Epoch: 53, Loss: 0.0046\n",
      "0.0009467601776123047\n",
      "Epoch: 54, Loss: 0.0046\n",
      "0.0009217262268066406\n",
      "Epoch: 55, Loss: 0.0050\n",
      "0.0009665489196777344\n",
      "Epoch: 56, Loss: 0.0044\n",
      "0.0009365081787109375\n",
      "Epoch: 57, Loss: 0.0045\n",
      "0.0009238719940185547\n",
      "Epoch: 58, Loss: 0.0042\n",
      "0.0009157657623291016\n",
      "Epoch: 59, Loss: 0.0045\n",
      "0.0009300708770751953\n",
      "Epoch: 60, Loss: 0.0048\n",
      "0.0009412765502929688\n",
      "Epoch: 61, Loss: 0.0047\n",
      "0.0009407997131347656\n",
      "Epoch: 62, Loss: 0.0045\n",
      "0.0009326934814453125\n",
      "Epoch: 63, Loss: 0.0045\n",
      "0.0009064674377441406\n",
      "Epoch: 64, Loss: 0.0053\n",
      "0.0009522438049316406\n",
      "Epoch: 65, Loss: 0.0046\n",
      "0.0009233951568603516\n",
      "Epoch: 66, Loss: 0.0047\n",
      "0.000942230224609375\n",
      "Epoch: 67, Loss: 0.0049\n",
      "0.0009419918060302734\n",
      "Epoch: 68, Loss: 0.0048\n",
      "0.0009343624114990234\n",
      "Epoch: 69, Loss: 0.0052\n",
      "0.0009365081787109375\n",
      "Epoch: 70, Loss: 0.0048\n",
      "0.000982522964477539\n",
      "Epoch: 71, Loss: 0.0048\n",
      "0.0009801387786865234\n",
      "Epoch: 72, Loss: 0.0048\n",
      "0.0009059906005859375\n",
      "Epoch: 73, Loss: 0.0049\n",
      "0.0009126663208007812\n",
      "Epoch: 74, Loss: 0.0052\n",
      "0.0009121894836425781\n",
      "Epoch: 75, Loss: 0.0047\n",
      "0.0009562969207763672\n",
      "Epoch: 76, Loss: 0.0049\n",
      "0.0009419918060302734\n",
      "Epoch: 77, Loss: 0.0052\n",
      "0.0009236335754394531\n",
      "Epoch: 78, Loss: 0.0048\n",
      "0.0009138584136962891\n",
      "Epoch: 79, Loss: 0.0050\n",
      "0.0008821487426757812\n",
      "Epoch: 80, Loss: 0.0048\n",
      "0.0009133815765380859\n",
      "Epoch: 81, Loss: 0.0051\n",
      "0.0008962154388427734\n",
      "Epoch: 82, Loss: 0.0048\n",
      "0.0008840560913085938\n",
      "Epoch: 83, Loss: 0.0046\n",
      "0.0009565353393554688\n",
      "Epoch: 84, Loss: 0.0055\n",
      "0.0009264945983886719\n",
      "Epoch: 85, Loss: 0.0052\n",
      "0.000919342041015625\n",
      "Epoch: 86, Loss: 0.0049\n",
      "0.0009143352508544922\n",
      "Epoch: 87, Loss: 0.0051\n",
      "0.0009474754333496094\n",
      "Epoch: 88, Loss: 0.0050\n",
      "0.0009307861328125\n",
      "Epoch: 89, Loss: 0.0041\n",
      "0.0009257793426513672\n",
      "Epoch: 90, Loss: 0.0045\n",
      "0.0010492801666259766\n",
      "Epoch: 91, Loss: 0.0048\n",
      "0.0010797977447509766\n",
      "Epoch: 92, Loss: 0.0045\n",
      "0.0009033679962158203\n",
      "Epoch: 93, Loss: 0.0049\n",
      "0.0009570121765136719\n",
      "Epoch: 94, Loss: 0.0057\n",
      "0.0009038448333740234\n",
      "Epoch: 95, Loss: 0.0045\n",
      "0.0009324550628662109\n",
      "Epoch: 96, Loss: 0.0050\n",
      "0.0009243488311767578\n",
      "Epoch: 97, Loss: 0.0050\n",
      "0.0010426044464111328\n",
      "Epoch: 98, Loss: 0.0044\n",
      "0.0008804798126220703\n",
      "Epoch: 99, Loss: 0.0046\n",
      "0.0009191036224365234\n",
      "Epoch: 100, Loss: 0.0054\n",
      "0.0009324550628662109\n",
      "Epoch: 101, Loss: 0.0047\n",
      "0.0009191036224365234\n",
      "Epoch: 102, Loss: 0.0052\n",
      "0.0008952617645263672\n",
      "Epoch: 103, Loss: 0.0046\n",
      "0.0009179115295410156\n",
      "Epoch: 104, Loss: 0.0047\n",
      "0.0009202957153320312\n",
      "Epoch: 105, Loss: 0.0043\n",
      "0.0009441375732421875\n",
      "Epoch: 106, Loss: 0.0050\n",
      "0.0009512901306152344\n",
      "Epoch: 107, Loss: 0.0044\n",
      "0.0009031295776367188\n",
      "Epoch: 108, Loss: 0.0049\n",
      "0.0009119510650634766\n",
      "Epoch: 109, Loss: 0.0048\n",
      "0.0009160041809082031\n",
      "Epoch: 110, Loss: 0.0045\n",
      "0.0009222030639648438\n",
      "Epoch: 111, Loss: 0.0051\n",
      "0.0009160041809082031\n",
      "Epoch: 112, Loss: 0.0048\n",
      "0.0009150505065917969\n",
      "Epoch: 113, Loss: 0.0046\n",
      "0.0009338855743408203\n",
      "Epoch: 114, Loss: 0.0044\n",
      "0.0009281635284423828\n",
      "Epoch: 115, Loss: 0.0047\n",
      "0.0009441375732421875\n",
      "Epoch: 116, Loss: 0.0043\n",
      "0.0009398460388183594\n",
      "Epoch: 117, Loss: 0.0049\n",
      "0.0009315013885498047\n",
      "Epoch: 118, Loss: 0.0051\n",
      "0.0008947849273681641\n",
      "Epoch: 119, Loss: 0.0048\n",
      "0.0008995532989501953\n",
      "Epoch: 120, Loss: 0.0042\n",
      "0.0009315013885498047\n",
      "Epoch: 121, Loss: 0.0046\n",
      "0.0008904933929443359\n",
      "Epoch: 122, Loss: 0.0041\n",
      "0.0009350776672363281\n",
      "Epoch: 123, Loss: 0.0051\n",
      "0.0009257793426513672\n",
      "Epoch: 124, Loss: 0.0049\n",
      "0.0008971691131591797\n",
      "Epoch: 125, Loss: 0.0046\n",
      "0.0009207725524902344\n",
      "Epoch: 126, Loss: 0.0051\n",
      "0.00090789794921875\n",
      "Epoch: 127, Loss: 0.0050\n",
      "0.0009202957153320312\n",
      "Epoch: 128, Loss: 0.0050\n",
      "0.0009074211120605469\n",
      "Epoch: 129, Loss: 0.0052\n",
      "0.000919342041015625\n",
      "Epoch: 130, Loss: 0.0052\n",
      "0.0010716915130615234\n",
      "Epoch: 131, Loss: 0.0046\n",
      "0.0009069442749023438\n",
      "Epoch: 132, Loss: 0.0046\n",
      "0.0008878707885742188\n",
      "Epoch: 133, Loss: 0.0047\n",
      "0.0008809566497802734\n",
      "Epoch: 134, Loss: 0.0051\n",
      "0.0009315013885498047\n",
      "Epoch: 135, Loss: 0.0042\n",
      "0.0008890628814697266\n",
      "Epoch: 136, Loss: 0.0047\n",
      "0.0009233951568603516\n",
      "Epoch: 137, Loss: 0.0047\n",
      "0.0009222030639648438\n",
      "Epoch: 138, Loss: 0.0046\n",
      "0.0009491443634033203\n",
      "Epoch: 139, Loss: 0.0047\n",
      "0.0009233951568603516\n",
      "Epoch: 140, Loss: 0.0045\n",
      "0.0009083747863769531\n",
      "Epoch: 141, Loss: 0.0049\n",
      "0.0009033679962158203\n",
      "Epoch: 142, Loss: 0.0044\n",
      "0.00107574462890625\n",
      "Epoch: 143, Loss: 0.0048\n",
      "0.0009000301361083984\n",
      "Epoch: 144, Loss: 0.0048\n",
      "0.0009343624114990234\n",
      "Epoch: 145, Loss: 0.0047\n",
      "0.0008852481842041016\n",
      "Epoch: 146, Loss: 0.0049\n",
      "0.0009217262268066406\n",
      "Epoch: 147, Loss: 0.0051\n",
      "0.0009291172027587891\n",
      "Epoch: 148, Loss: 0.0048\n",
      "0.0009071826934814453\n",
      "Epoch: 149, Loss: 0.0053\n",
      "0.0009448528289794922\n",
      "Epoch: 150, Loss: 0.0049\n",
      "0.0009591579437255859\n",
      "Epoch: 151, Loss: 0.0049\n",
      "0.0009067058563232422\n",
      "Epoch: 152, Loss: 0.0047\n",
      "0.0009496212005615234\n",
      "Epoch: 153, Loss: 0.0055\n",
      "0.0009059906005859375\n",
      "Epoch: 154, Loss: 0.0047\n",
      "0.0010344982147216797\n",
      "Epoch: 155, Loss: 0.0048\n",
      "0.0009906291961669922\n",
      "Epoch: 156, Loss: 0.0052\n",
      "0.0009109973907470703\n",
      "Epoch: 157, Loss: 0.0046\n",
      "0.0009298324584960938\n",
      "Epoch: 158, Loss: 0.0047\n",
      "0.0009281635284423828\n",
      "Epoch: 159, Loss: 0.0049\n",
      "0.0009005069732666016\n",
      "Epoch: 160, Loss: 0.0048\n",
      "0.0009257793426513672\n",
      "Epoch: 161, Loss: 0.0046\n",
      "0.0009083747863769531\n",
      "Epoch: 162, Loss: 0.0046\n",
      "0.0009167194366455078\n",
      "Epoch: 163, Loss: 0.0053\n",
      "0.0009145736694335938\n",
      "Epoch: 164, Loss: 0.0052\n",
      "0.0009257793426513672\n",
      "Epoch: 165, Loss: 0.0046\n",
      "0.0009133815765380859\n",
      "Epoch: 166, Loss: 0.0050\n",
      "0.0009341239929199219\n",
      "Epoch: 167, Loss: 0.0046\n",
      "0.0009191036224365234\n",
      "Epoch: 168, Loss: 0.0040\n",
      "0.0009064674377441406\n",
      "Epoch: 169, Loss: 0.0047\n",
      "0.0009183883666992188\n",
      "Epoch: 170, Loss: 0.0042\n",
      "0.0008900165557861328\n",
      "Epoch: 171, Loss: 0.0048\n",
      "0.0009315013885498047\n",
      "Epoch: 172, Loss: 0.0055\n",
      "0.0009214878082275391\n",
      "Epoch: 173, Loss: 0.0046\n",
      "0.0009107589721679688\n",
      "Epoch: 174, Loss: 0.0055\n",
      "0.0009059906005859375\n",
      "Epoch: 175, Loss: 0.0052\n",
      "0.0009374618530273438\n",
      "Epoch: 176, Loss: 0.0046\n",
      "0.0008630752563476562\n",
      "Epoch: 177, Loss: 0.0045\n",
      "0.0009276866912841797\n",
      "Epoch: 178, Loss: 0.0049\n",
      "0.0009071826934814453\n",
      "Epoch: 179, Loss: 0.0045\n",
      "0.0009362697601318359\n",
      "Epoch: 180, Loss: 0.0044\n",
      "0.0009012222290039062\n",
      "Epoch: 181, Loss: 0.0051\n",
      "0.0009334087371826172\n",
      "Epoch: 182, Loss: 0.0053\n",
      "0.000942230224609375\n",
      "Epoch: 183, Loss: 0.0051\n",
      "0.0009341239929199219\n",
      "Epoch: 184, Loss: 0.0046\n",
      "0.0009741783142089844\n",
      "Epoch: 185, Loss: 0.0053\n",
      "0.0009317398071289062\n",
      "Epoch: 186, Loss: 0.0047\n",
      "0.0009326934814453125\n",
      "Epoch: 187, Loss: 0.0045\n",
      "0.0009131431579589844\n",
      "Epoch: 188, Loss: 0.0046\n",
      "0.0009615421295166016\n",
      "Epoch: 189, Loss: 0.0046\n",
      "0.0009129047393798828\n",
      "Epoch: 190, Loss: 0.0053\n",
      "0.0009319782257080078\n",
      "Epoch: 191, Loss: 0.0048\n",
      "0.0009076595306396484\n",
      "Epoch: 192, Loss: 0.0051\n",
      "0.0009121894836425781\n",
      "Epoch: 193, Loss: 0.0049\n",
      "0.0009143352508544922\n",
      "Epoch: 194, Loss: 0.0049\n",
      "0.00091552734375\n",
      "Epoch: 195, Loss: 0.0047\n",
      "0.0008945465087890625\n",
      "Epoch: 196, Loss: 0.0046\n",
      "0.0010323524475097656\n",
      "Epoch: 197, Loss: 0.0049\n",
      "0.0009341239929199219\n",
      "Epoch: 198, Loss: 0.0047\n",
      "0.00091552734375\n",
      "Epoch: 199, Loss: 0.0051\n",
      "0.0009038448333740234\n",
      "Epoch: 200, Loss: 0.0053\n",
      "0.0010607242584228516\n",
      "Epoch: 201, Loss: 0.0051\n",
      "0.0009324550628662109\n",
      "Epoch: 202, Loss: 0.0048\n",
      "0.0009322166442871094\n",
      "Epoch: 203, Loss: 0.0043\n",
      "0.0009107589721679688\n",
      "Epoch: 204, Loss: 0.0048\n",
      "0.0009086132049560547\n",
      "Epoch: 205, Loss: 0.0048\n",
      "0.0009319782257080078\n",
      "Epoch: 206, Loss: 0.0050\n",
      "0.0009367465972900391\n",
      "Epoch: 207, Loss: 0.0045\n",
      "0.0009772777557373047\n",
      "Epoch: 208, Loss: 0.0046\n",
      "0.0009348392486572266\n",
      "Epoch: 209, Loss: 0.0045\n",
      "0.0008778572082519531\n",
      "Epoch: 210, Loss: 0.0052\n",
      "0.0009071826934814453\n",
      "Epoch: 211, Loss: 0.0048\n",
      "0.0009386539459228516\n",
      "Epoch: 212, Loss: 0.0041\n",
      "0.0009055137634277344\n",
      "Epoch: 213, Loss: 0.0050\n",
      "0.0009357929229736328\n",
      "Epoch: 214, Loss: 0.0047\n",
      "0.0009639263153076172\n",
      "Epoch: 215, Loss: 0.0045\n",
      "0.0009272098541259766\n",
      "Epoch: 216, Loss: 0.0050\n",
      "0.0009083747863769531\n",
      "Epoch: 217, Loss: 0.0050\n",
      "0.0009303092956542969\n",
      "Epoch: 218, Loss: 0.0052\n",
      "0.0009355545043945312\n",
      "Epoch: 219, Loss: 0.0047\n",
      "0.0009405612945556641\n",
      "Epoch: 220, Loss: 0.0041\n",
      "0.0009584426879882812\n",
      "Epoch: 221, Loss: 0.0051\n",
      "0.0009362697601318359\n",
      "Epoch: 222, Loss: 0.0051\n",
      "0.0009021759033203125\n",
      "Epoch: 223, Loss: 0.0048\n",
      "0.0009427070617675781\n",
      "Epoch: 224, Loss: 0.0048\n",
      "0.000926971435546875\n",
      "Epoch: 225, Loss: 0.0050\n",
      "0.0009891986846923828\n",
      "Epoch: 226, Loss: 0.0045\n",
      "0.0009074211120605469\n",
      "Epoch: 227, Loss: 0.0041\n",
      "0.0009553432464599609\n",
      "Epoch: 228, Loss: 0.0046\n",
      "0.0009253025054931641\n",
      "Epoch: 229, Loss: 0.0040\n",
      "0.0008800029754638672\n",
      "Epoch: 230, Loss: 0.0053\n",
      "0.0009160041809082031\n",
      "Epoch: 231, Loss: 0.0049\n",
      "0.000896453857421875\n",
      "Epoch: 232, Loss: 0.0048\n",
      "0.0009083747863769531\n",
      "Epoch: 233, Loss: 0.0050\n",
      "0.0008802413940429688\n",
      "Epoch: 234, Loss: 0.0048\n",
      "0.0009024143218994141\n",
      "Epoch: 235, Loss: 0.0043\n",
      "0.000919342041015625\n",
      "Epoch: 236, Loss: 0.0046\n",
      "0.0009331703186035156\n",
      "Epoch: 237, Loss: 0.0049\n",
      "0.0009949207305908203\n",
      "Epoch: 238, Loss: 0.0051\n",
      "0.0009107589721679688\n",
      "Epoch: 239, Loss: 0.0049\n",
      "0.0009007453918457031\n",
      "Epoch: 240, Loss: 0.0058\n",
      "0.0009338855743408203\n",
      "Epoch: 241, Loss: 0.0052\n",
      "0.0009176731109619141\n",
      "Epoch: 242, Loss: 0.0045\n",
      "0.0009324550628662109\n",
      "Epoch: 243, Loss: 0.0054\n",
      "0.0009925365447998047\n",
      "Epoch: 244, Loss: 0.0049\n",
      "0.0009248256683349609\n",
      "Epoch: 245, Loss: 0.0047\n",
      "0.0010526180267333984\n",
      "Epoch: 246, Loss: 0.0050\n",
      "0.0008890628814697266\n",
      "Epoch: 247, Loss: 0.0052\n",
      "0.0009007453918457031\n",
      "Epoch: 248, Loss: 0.0050\n",
      "0.0008969306945800781\n",
      "Epoch: 249, Loss: 0.0046\n",
      "0.0009093284606933594\n",
      "Epoch: 250, Loss: 0.0049\n",
      "0.0009465217590332031\n",
      "Epoch: 251, Loss: 0.0051\n",
      "0.0009202957153320312\n",
      "Epoch: 252, Loss: 0.0051\n",
      "0.0008914470672607422\n",
      "Epoch: 253, Loss: 0.0050\n",
      "0.0009214878082275391\n",
      "Epoch: 254, Loss: 0.0052\n",
      "0.0009160041809082031\n",
      "Epoch: 255, Loss: 0.0044\n",
      "0.0009319782257080078\n",
      "Epoch: 256, Loss: 0.0043\n",
      "0.0009212493896484375\n",
      "Epoch: 257, Loss: 0.0054\n",
      "0.0009300708770751953\n",
      "Epoch: 258, Loss: 0.0049\n",
      "0.0009293556213378906\n",
      "Epoch: 259, Loss: 0.0050\n",
      "0.0009150505065917969\n",
      "Epoch: 260, Loss: 0.0049\n",
      "0.0009086132049560547\n",
      "Epoch: 261, Loss: 0.0046\n",
      "0.0009162425994873047\n",
      "Epoch: 262, Loss: 0.0043\n",
      "0.0008959770202636719\n",
      "Epoch: 263, Loss: 0.0046\n",
      "0.0009584426879882812\n",
      "Epoch: 264, Loss: 0.0053\n",
      "0.0009202957153320312\n",
      "Epoch: 265, Loss: 0.0047\n",
      "0.0009746551513671875\n",
      "Epoch: 266, Loss: 0.0048\n",
      "0.0009419918060302734\n",
      "Epoch: 267, Loss: 0.0043\n",
      "0.0009279251098632812\n",
      "Epoch: 268, Loss: 0.0049\n",
      "0.0009565353393554688\n",
      "Epoch: 269, Loss: 0.0050\n",
      "0.0009436607360839844\n",
      "Epoch: 270, Loss: 0.0047\n",
      "0.0009293556213378906\n",
      "Epoch: 271, Loss: 0.0043\n",
      "0.0008909702301025391\n",
      "Epoch: 272, Loss: 0.0049\n",
      "0.0009226799011230469\n",
      "Epoch: 273, Loss: 0.0050\n",
      "0.0008790493011474609\n",
      "Epoch: 274, Loss: 0.0048\n",
      "0.0008797645568847656\n",
      "Epoch: 275, Loss: 0.0047\n",
      "0.000926971435546875\n",
      "Epoch: 276, Loss: 0.0046\n",
      "0.0010762214660644531\n",
      "Epoch: 277, Loss: 0.0046\n",
      "0.0009031295776367188\n",
      "Epoch: 278, Loss: 0.0047\n",
      "0.0009026527404785156\n",
      "Epoch: 279, Loss: 0.0050\n",
      "0.0009262561798095703\n",
      "Epoch: 280, Loss: 0.0051\n",
      "0.0008962154388427734\n",
      "Epoch: 281, Loss: 0.0055\n",
      "0.0009653568267822266\n",
      "Epoch: 282, Loss: 0.0048\n",
      "0.0009107589721679688\n",
      "Epoch: 283, Loss: 0.0048\n",
      "0.0014717578887939453\n",
      "Epoch: 284, Loss: 0.0049\n",
      "0.0008854866027832031\n",
      "Epoch: 285, Loss: 0.0046\n",
      "0.0009188652038574219\n",
      "Epoch: 286, Loss: 0.0046\n",
      "0.0009675025939941406\n",
      "Epoch: 287, Loss: 0.0053\n",
      "0.0009014606475830078\n",
      "Epoch: 288, Loss: 0.0055\n",
      "0.0009348392486572266\n",
      "Epoch: 289, Loss: 0.0051\n",
      "0.0009167194366455078\n",
      "Epoch: 290, Loss: 0.0046\n",
      "0.0009200572967529297\n",
      "Epoch: 291, Loss: 0.0047\n",
      "0.0009174346923828125\n",
      "Epoch: 292, Loss: 0.0048\n",
      "0.0009133815765380859\n",
      "Epoch: 293, Loss: 0.0051\n",
      "0.0009095668792724609\n",
      "Epoch: 294, Loss: 0.0046\n",
      "0.0008966922760009766\n",
      "Epoch: 295, Loss: 0.0048\n",
      "0.0009129047393798828\n",
      "Epoch: 296, Loss: 0.0056\n",
      "0.0009064674377441406\n",
      "Epoch: 297, Loss: 0.0051\n",
      "0.0009093284606933594\n",
      "Epoch: 298, Loss: 0.0051\n",
      "0.0009129047393798828\n",
      "Epoch: 299, Loss: 0.0048\n",
      "0.0011043548583984375\n",
      "Epoch: 300, Loss: 0.0050\n",
      "0.0008714199066162109\n",
      "Epoch: 301, Loss: 0.0042\n",
      "0.0008976459503173828\n",
      "Epoch: 302, Loss: 0.0045\n",
      "0.0009081363677978516\n",
      "Epoch: 303, Loss: 0.0044\n",
      "0.0009512901306152344\n",
      "Epoch: 304, Loss: 0.0047\n",
      "0.0009195804595947266\n",
      "Epoch: 305, Loss: 0.0049\n",
      "0.0009210109710693359\n",
      "Epoch: 306, Loss: 0.0048\n",
      "0.0009620189666748047\n",
      "Epoch: 307, Loss: 0.0053\n",
      "0.0009107589721679688\n",
      "Epoch: 308, Loss: 0.0053\n",
      "0.0009100437164306641\n",
      "Epoch: 309, Loss: 0.0049\n",
      "0.0008840560913085938\n",
      "Epoch: 310, Loss: 0.0046\n",
      "0.0009191036224365234\n",
      "Epoch: 311, Loss: 0.0048\n",
      "0.0008943080902099609\n",
      "Epoch: 312, Loss: 0.0051\n",
      "0.000949859619140625\n",
      "Epoch: 313, Loss: 0.0044\n",
      "0.0008895397186279297\n",
      "Epoch: 314, Loss: 0.0048\n",
      "0.0009529590606689453\n",
      "Epoch: 315, Loss: 0.0046\n",
      "0.0008883476257324219\n",
      "Epoch: 316, Loss: 0.0045\n",
      "0.0009243488311767578\n",
      "Epoch: 317, Loss: 0.0052\n",
      "0.0009129047393798828\n",
      "Epoch: 318, Loss: 0.0055\n",
      "0.0009062290191650391\n",
      "Epoch: 319, Loss: 0.0047\n",
      "0.0009396076202392578\n",
      "Epoch: 320, Loss: 0.0049\n",
      "0.0009250640869140625\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "num_epochs = 320\n",
    "outputs=[]\n",
    "for epoch in range(num_epochs):\n",
    "    for img, _ in data_loader:\n",
    "        bruh = time.time()\n",
    "        img = img.to(device)\n",
    "        recon = model(img)\n",
    "        loss = criterion(recon, img)\n",
    "        waaa = time.time()\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f\"Epoch: {epoch+1}, Loss: {loss.item():.4f}\")\n",
    "    outputs.append((epoch, img, recon))\n",
    "    print(waaa-bruh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"./models/conv-auto-encoder.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ConvAutoencoder()\n",
    "model.load_state_dict(torch.load(\"./models/conv-auto-encoder.pth\"))\n",
    "model.eval()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
